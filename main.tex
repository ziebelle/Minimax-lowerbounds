\documentclass{book}
\usepackage{graphicx} % Required for inserting images

\usepackage{preamble}

\author{Eric Ziebell}
\title{Gaussian minimax lowerbounds}
\begin{document}
\maketitle

%latexindent -w main.tex
{\color{red} Warning: This document is dedigated to my personal learning process, and might contain errors.}
\chapter{Minimax lowerbounds}
\begin{definition}
	Let $(\mathcal{X}_n, \mathcal{F}_n, (\mathbb{P}_{\vartheta,n})_{\vartheta \in \Theta})$ be a statistical model and $(\Theta, d)$ a metric space\footnote{In general $d:\Theta \times \Theta \rightarrow [0,\infty)$ is also allowed to be a semi-distance.}. Suppose that $(v_n)_{n \in \mathbb{N}}$ is a null sequence. Then, $(v_n)_{n \in \mathbb{N}}$ is called optimal (minimax) convergence rate over $\Theta$ if
	\begin{enumerate}
		\item There exists an estimator $\hat{\vartheta}_n^{*}$ such that
		      \begin{equation*}
			      \label{condition:upperbound}
			      \limsup_{n \rightarrow \infty} v_n^{-2} \sup_{\vartheta \in \Theta} \mathbb{E}_{\vartheta, n}[d(\hat{\vartheta}_n^{*}, \vartheta)^{2}]< \infty.
		      \end{equation*}
		\item We have the uniform lowerbound
		      \begin{equation*}
			      \liminf_{n \rightarrow \infty} v_n^{-2} \inf_{\hat{\vartheta}_n} \sup_{\vartheta \in \Theta} \mathbb{E}_{\vartheta,n}[d(\hat{\vartheta}_{n}, \vartheta)^{2}]>0,
		      \end{equation*}
		      where the infimum is taken over all measurable functions (estimators) in model $n$.
	\end{enumerate}
\end{definition}

\begin{proposition}[Reduction scheme to a testing problem]
	\label{result:reduction_scheme}
	Let $(\mathcal{X}_n, \mathcal{F}_n, (\mathbb{P}_{\vartheta,n})_{\vartheta \in \Theta})$ be a statistical model and $(\Theta, d)$ a metric space. Suppose that $(v_n)_{n \in \mathbb{N}}$ is a null sequence satisfying an upper bound \eqref{condition:upperbound} and
	\begin{equation*}
		\liminf_{n \rightarrow \infty} \inf_{\psi_n} \max_{j=1,\dots,M} \mathbb{P}_{\vartheta_j,n}(\psi_n \neq j)>0.
	\end{equation*}
	Then, $v_n$ is the optimal minimax convergence rate over $\Theta$.
\end{proposition}
\begin{proof}[Proof of \Cref{result:reduction_scheme}]
	\begin{enumerate}
		\item[]
		\item With Markov's inequality, we observe that forall $\alpha>0$, we have
		      \begin{equation*}
			      v_{n}^{-2}\mathbb{E}_{\vartheta,n}[d(\hat{\vartheta}_n, \vartheta)^{2}] \geq \alpha^{2}\mathbb{P}_{\vartheta,n}(d(\hat{\vartheta}_n, \vartheta) \geq \alpha v_n),
		      \end{equation*}
		      such that
		      \begin{equation*}
			      \liminf_{n \rightarrow \infty} v_n^{-2} \inf_{\hat{\vartheta}_n} \sup_{\vartheta \in \Theta} \mathbb{E}_{\vartheta,n}[d(\hat{\vartheta}_{n}, \vartheta)^{2}] \geq \liminf_{n \rightarrow \infty} \inf_{\hat{\vartheta}_n} \sup_{\vartheta \in \Theta} \alpha^{2}\mathbb{P}_{\vartheta,n}(d(\hat{\vartheta}_n, \vartheta) \geq \alpha v_n).
		      \end{equation*}
		\item For any subset $\{\vartheta_1, \dots, \vartheta_M\} \subset \Theta$, we obtain the lowerbound
		      \begin{equation*}
			      \liminf_{n \rightarrow \infty} \inf_{\hat{\vartheta}_n} \sup_{\vartheta \in \Theta} \alpha^{2}\mathbb{P}_{\vartheta,n}(d(\hat{\vartheta}_n, \vartheta) \geq \alpha v_n) \geq \liminf_{n \rightarrow \infty} \inf_{\hat{\vartheta}_n} \max_{j=1,\dots,M} \alpha^{2}\mathbb{P}_{\vartheta_j,n}(d(\hat{\vartheta}_n, \vartheta_j) \geq \alpha v_n).
		      \end{equation*}
		\item Suppose that $\{\vartheta_1, \dots, \vartheta_M\} \subset \Theta$ are seperated according to
		      \begin{equation*}
			      d(\vartheta_i, \vartheta_j)> 2 \alpha v_n = \gamma, \quad i \neq j, \quad i,j=1, \dots, M.
		      \end{equation*}
		      Let us now consider the minimum distance test $\psi^{*}:\mathcal{X}_n \rightarrow \{1, \dots,M\}$:
		      \begin{equation*}
			      \psi^{*}=\mathrm{argmin}_{k=1, \dots, M} d(\hat{\vartheta}_n, \vartheta_k).
		      \end{equation*}
		      Since the hypothesis are seperated with radius $\gamma$, we observe that $\{\hat{\vartheta}_n \in B_{\gamma}(\vartheta_j)\} \subset\{\psi^{*} = j\}$. Thus, we clearly have the inclusion $\{\psi^{*} \neq j\}  \subset \{ \hat{\vartheta}_n  \in B_{\gamma}(\vartheta_j)\}^{c}=\{\hat{\vartheta}_n \in B_{\gamma}(\vartheta_j)^{c}\}$. However, it is still possible that $\hat{\vartheta}_n$ is the closest to $\vartheta_j$ but not inside of a $\gamma$ ball, so the inclusion might be strict. All in all, we obtain the bound
		      \begin{equation*}
			      \mathbb{P}_{\vartheta_j,n}(d(\hat{\vartheta}_n, \vartheta_j) \geq \gamma) \geq \mathbb{P}_{\vartheta_j, n}(\psi^{*}\neq j), \quad j=1, \dots,M.
		      \end{equation*}
		\item Since for any $\hat{\vartheta}_n$, we can construct such a test, we may replace the infimum over all estimators with an infimum over all tests in model $n$, yielding the inequality
		      \begin{equation*}
			      \liminf_{n \rightarrow \infty} \inf_{\hat{\vartheta}_n} \max_{j=1,\dots,M} \alpha^{2}\mathbb{P}_{\vartheta_j,n}(d(\hat{\vartheta}_n, \vartheta_j) \geq \alpha v_n) \geq \alpha^{2}  \liminf_{n \rightarrow \infty} \inf_{\psi_n} \max_{j=1,\dots,M} \mathbb{P}_{\vartheta_j,n}(\psi_n \neq j).
		      \end{equation*}
		      Thus, if the latter term is positive we obtain the desired lowerbound. \qedhere
	\end{enumerate}
\end{proof}




\bibliographystyle{plainnat}
\bibliography{zotero_28.04.2025}
\end{document}
