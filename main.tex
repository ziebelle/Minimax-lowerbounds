%\documentclass{book}
\documentclass[a4paper]{article}
\usepackage{graphicx} % Required for inserting images

\usepackage{preamble}

\author{Eric Ziebell}
\title{Minimax lowerbounds}
\begin{document}
\maketitle

%latexindent -w main.tex
\paragraph{General comments}
\begin{enumerate}
\item Warning: This document is dedicated to my personal learning process, and might contain errors.
\item If you find any typos or errors, feel free to make a pull request or contact me directly. 
\end{enumerate}
\section{The essentials}
Most of the content from this section can be found in \citet[Chapter 2]{tsybakovIntroductionNonparametricEstimation2009}.
\begin{definition}
	Let $(\mathcal{X}_n, \mathcal{F}_n, (\mathbb{P}_{\vartheta,n})_{\vartheta \in \Theta})$ be a statistical model and $(\Theta, d)$ a metric space\footnote{In general $d:\Theta \times \Theta \rightarrow [0,\infty)$ is also allowed to be a semi-distance.}. Suppose that $(v_n)_{n \in \mathbb{N}}$ is a null sequence. Then, $(v_n)_{n \in \mathbb{N}}$ is called optimal (minimax) convergence rate over $\Theta$ if
	\begin{enumerate}
		\item There exists an estimator $\hat{\vartheta}_n^{*}$ such that
		      \begin{equation*}
			      \label{condition:upperbound}
			      \limsup_{n \rightarrow \infty} v_n^{-2} \sup_{\vartheta \in \Theta} \mathbb{E}_{\vartheta, n}[d(\hat{\vartheta}_n^{*}, \vartheta)^{2}]< \infty.
		      \end{equation*}
		\item We have the uniform lowerbound
		      \begin{equation*}
			      \liminf_{n \rightarrow \infty} v_n^{-2} \inf_{\hat{\vartheta}_n} \sup_{\vartheta \in \Theta} \mathbb{E}_{\vartheta,n}[d(\hat{\vartheta}_{n}, \vartheta)^{2}]>0,
		      \end{equation*}
		      where the infimum is taken over all measurable functions (estimators) in model $n$.
	\end{enumerate}
\end{definition}

\begin{propositionrep}[Reduction scheme to a testing problem]
	\label{result:reduction_scheme}
	Let $(\mathcal{X}_n, \mathcal{F}_n, (\mathbb{P}_{\vartheta,n})_{\vartheta \in \Theta})$ be a statistical model and $(\Theta, d)$ a metric space. Suppose that $(v_n)_{n \in \mathbb{N}}$ is a null sequence satisfying an upper bound \eqref{condition:upperbound} and
	\begin{equation*}
		\liminf_{n \rightarrow \infty} \inf_{\psi_n} \max_{j=1,\dots,M} \mathbb{P}_{\vartheta_j,n}(\psi_n \neq j)>0,
	\end{equation*}
    for parameters $\{\vartheta_1, \dots, \vartheta_M\} \subset \Theta$ separated according to $(v_n)_{n \in \mathbb{N}}$, see \eqref{eq:seperation_condition}.	Then, $v_n$ is the optimal minimax convergence rate over $\Theta$.
\end{propositionrep}
\begin{proof}[Proof of \Cref{result:reduction_scheme}]
	\begin{enumerate}
		\item[]
		\item With Markov's inequality, we observe that forall $\alpha>0$, we have
		      \begin{equation*}
			      v_{n}^{-2}\mathbb{E}_{\vartheta,n}[d(\hat{\vartheta}_n, \vartheta)^{2}] \geq \alpha^{2}\mathbb{P}_{\vartheta,n}(d(\hat{\vartheta}_n, \vartheta) \geq \alpha v_n),
		      \end{equation*}
		      such that
		      \begin{equation*}
			      \liminf_{n \rightarrow \infty} v_n^{-2} \inf_{\hat{\vartheta}_n} \sup_{\vartheta \in \Theta} \mathbb{E}_{\vartheta,n}[d(\hat{\vartheta}_{n}, \vartheta)^{2}] \geq \liminf_{n \rightarrow \infty} \inf_{\hat{\vartheta}_n} \sup_{\vartheta \in \Theta} \alpha^{2}\mathbb{P}_{\vartheta,n}(d(\hat{\vartheta}_n, \vartheta) \geq \alpha v_n).
		      \end{equation*}
		\item For any subset $\{\vartheta_1, \dots, \vartheta_M\} \subset \Theta$, we obtain the lowerbound
		      \begin{equation*}
			      \liminf_{n \rightarrow \infty} \inf_{\hat{\vartheta}_n} \sup_{\vartheta \in \Theta} \alpha^{2}\mathbb{P}_{\vartheta,n}(d(\hat{\vartheta}_n, \vartheta) \geq \alpha v_n) \geq \liminf_{n \rightarrow \infty} \inf_{\hat{\vartheta}_n} \max_{j=1,\dots,M} \alpha^{2}\mathbb{P}_{\vartheta_j,n}(d(\hat{\vartheta}_n, \vartheta_j) \geq \alpha v_n).
		      \end{equation*}
		\item Suppose that $\{\vartheta_1, \dots, \vartheta_M\} \subset \Theta$ are seperated according to
		      \begin{equation}
                    \label{eq:seperation_condition}
			      d(\vartheta_i, \vartheta_j)> 2 \alpha v_n = \gamma, \quad i \neq j, \quad i,j=1, \dots, M.
		      \end{equation}
		      Let us now consider the minimum distance test $\psi^{*}:\mathcal{X}_n \rightarrow \{1, \dots,M\}$:
		      \begin{equation*}
			      \psi^{*}=\mathrm{argmin}_{k=1, \dots, M} d(\hat{\vartheta}_n, \vartheta_k).
		      \end{equation*}
		      Since the hypothesis are seperated with radius $\gamma$, we observe that $\{\hat{\vartheta}_n \in B_{\gamma}(\vartheta_j)\} \subset\{\psi^{*} = j\}$. Thus, we clearly have the inclusion $\{\psi^{*} \neq j\}  \subset \{ \hat{\vartheta}_n  \in B_{\gamma}(\vartheta_j)\}^{c}=\{\hat{\vartheta}_n \in B_{\gamma}(\vartheta_j)^{c}\}$. However, it is still possible that $\hat{\vartheta}_n$ is the closest to $\vartheta_j$ but not inside of a $\gamma$ ball, so the inclusion might be strict. All in all, we obtain the bound
		      \begin{equation*}
			      \mathbb{P}_{\vartheta_j,n}(d(\hat{\vartheta}_n, \vartheta_j) \geq \gamma) \geq \mathbb{P}_{\vartheta_j, n}(\psi^{*}\neq j), \quad j=1, \dots,M.
		      \end{equation*}
		\item Since for any $\hat{\vartheta}_n$, we can construct such a test, we may replace the infimum over all estimators with an infimum over all tests in model $n$, yielding the inequality
		      \begin{equation*}
			      \liminf_{n \rightarrow \infty} \inf_{\hat{\vartheta}_n} \max_{j=1,\dots,M} \alpha^{2}\mathbb{P}_{\vartheta_j,n}(d(\hat{\vartheta}_n, \vartheta_j) \geq \alpha v_n) \geq \alpha^{2}  \liminf_{n \rightarrow \infty} \inf_{\psi_n} \max_{j=1,\dots,M} \mathbb{P}_{\vartheta_j,n}(\psi_n \neq j).
		      \end{equation*}
		      Thus, if the latter term is positive, we obtain the desired lower bound. \qedhere
	\end{enumerate}
\end{proof}
Let us first consider the situation where we consider only two hypotheses for the reduction from \Cref{result:reduction_scheme}. 
\begin{lemmarep}
Let $(\mathcal{X}_n, \mathcal{F}_n, (\mathbb{P}_{\vartheta,n})_{\vartheta \in \Theta})$ be a statistical model and $(\Theta,d)$ be a metric space. Suppose that $(v_n)_{n \in \mathbb{N}}$ is a null sequence satisfying an upper bound \eqref{condition:upperbound} and
\begin{equation*}
\liminf_{n \rightarrow \infty} \frac{1}{2}(1-\Vert \mathbb{P}_{\vartheta_1,n} -  \mathbb{P}_{\vartheta_1 + 2\alpha v_n, n}\Vert_{\mathrm{TV}})>0.
\end{equation*}
Then, $v_n$ is the optimal minimax convergence rate over $\Theta$. 
\end{lemmarep}
\begin{proof}
The result follows from \Cref{result:reduction_scheme} for $M=2$, by setting $\vartheta_2=\vartheta_1 + 2 \alpha v_n$ and observing
\begin{equation*}
\liminf_{n\rightarrow \infty} \inf_{\psi_n} \max_{j=1,2}\mathbb{P}_{n,\vartheta_j}(\psi_n \neq j) \geq \liminf_{n\rightarrow \infty} \inf_{\psi_n} \frac{1}{2}\left(\mathbb{P}_{\vartheta_1,n}(\psi_n = 2) + \mathbb{P}_{\vartheta_2, n}(\psi_n = 1)\right),
\end{equation*}
where we have used that $\max(a,b) \geq \frac{1}{2}(a+b)$. By going over to the complementary event, we obtain
\begin{equation*}
\begin{aligned}
&\frac{1}{2}\left(\mathbb{P}_{\vartheta_1,n}(\psi_n = 2) + \mathbb{P}_{\vartheta_2, n}(\psi_n = 1)\right) \\
&=\frac{1}{2}\left( 1 - \mathbb{P}_{\vartheta_1,n}(\psi_n=1) + 1- \mathbb{P}_{\vartheta_2,n}(\psi_n=2)\right)\\
&=1 - \frac{1}{2}\left(\mathbb{P}_{\vartheta_1,n}(\psi_n=1)+ \mathbb{P}_{\vartheta_2,n}(\psi_n=2)\right)\\
&=1 - \int_{\mathcal{X}_n} \frac{1}{2}(\mathbbm{1}_{\{\psi_{n}(x)=1\}
}p_1(x) + \mathbbm{1}_{\{\psi_{n}(x)=2\}
}p_2(x) ) \mathrm{d}\mu(x)\\
&\geq 1 - \frac{1}{2}\int_{\mathcal{X}_n} \max(p_0(x), p_1(x))\mathrm{d}\mu(x). \\
&= \frac{1}{2}(1-\Vert \mathbb{P}_{\vartheta_1,n} -  \mathbb{P}_{\vartheta_1 + 2\alpha v_n, n}\Vert_{\mathrm{TV}}),
\end{aligned}
\end{equation*}
concluding the proof. 
\end{proof}




\bibliographystyle{plainnat}
\bibliography{zotero_28.04.2025}
\end{document}
